{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac93753c",
   "metadata": {},
   "source": [
    "https://youtu.be/u--UVvH-LIQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafce58",
   "metadata": {},
   "source": [
    "# Выбираем пакет данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699a6a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Набор данных Вопросы-Ответы\n",
    "# Вопросы-Ответы (Question Answering)\n",
    "# Задача для практики с моделями, которые должны найти ответ в контексте (extractive QA).\n",
    "dataset_squad = load_dataset(\"squad\")\n",
    "dataset_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a2baf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f450d01",
   "metadata": {},
   "source": [
    "Разбиваю чать Train на Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf06e7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 78839\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 8760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Извлекаем существующий спилит\n",
    "train_data = dataset_squad[\"train\"]\n",
    "\n",
    "# 2. Разбиваем на два сплита\n",
    "# test_size=0.1\n",
    "# seeseed=42 для воспроизводимости\n",
    "test_split = train_data.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# 3. Обновляем исходный объект DatasetDict\n",
    "dataset_squad[\"train\"] = test_split[\"train\"]\n",
    "dataset_squad[\"test\"] = test_split[\"test\"]\n",
    "\n",
    "dataset_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00979e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57263127ec44d21400f3dbf9</td>\n",
       "      <td>Korean_War</td>\n",
       "      <td>After the formation of the People's Republic o...</td>\n",
       "      <td>To show their strength in the international Co...</td>\n",
       "      <td>{'text': ['promoted Communist revolutions'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733f93e4776f41900661602</td>\n",
       "      <td>Punjab,_Pakistan</td>\n",
       "      <td>There are 48 departments in Punjab government....</td>\n",
       "      <td>Who heads each government department?</td>\n",
       "      <td>{'text': ['a Provincial Minister (Politician) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56bfae97a10cfb1400551236</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé and husband Jay Z are friends with Pre...</td>\n",
       "      <td>What did they attend in July 2013?</td>\n",
       "      <td>{'text': ['a rally'], 'answer_start': [840]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bfa087a10cfb14005511da</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>On January 7, 2012, Beyoncé gave birth to her ...</td>\n",
       "      <td>What was the child's name?</td>\n",
       "      <td>{'text': ['Blue Ivy Carter'], 'answer_start': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570ceadbfed7b91900d45ad5</td>\n",
       "      <td>Gymnastics</td>\n",
       "      <td>General gymnastics enables people of all ages ...</td>\n",
       "      <td>What kind of routines do general gymnastic gro...</td>\n",
       "      <td>{'text': ['synchronized, choreographed routine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id             title  \\\n",
       "0  57263127ec44d21400f3dbf9        Korean_War   \n",
       "1  5733f93e4776f41900661602  Punjab,_Pakistan   \n",
       "2  56bfae97a10cfb1400551236           Beyoncé   \n",
       "3  56bfa087a10cfb14005511da           Beyoncé   \n",
       "4  570ceadbfed7b91900d45ad5        Gymnastics   \n",
       "\n",
       "                                             context  \\\n",
       "0  After the formation of the People's Republic o...   \n",
       "1  There are 48 departments in Punjab government....   \n",
       "2  Beyoncé and husband Jay Z are friends with Pre...   \n",
       "3  On January 7, 2012, Beyoncé gave birth to her ...   \n",
       "4  General gymnastics enables people of all ages ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To show their strength in the international Co...   \n",
       "1              Who heads each government department?   \n",
       "2                 What did they attend in July 2013?   \n",
       "3                         What was the child's name?   \n",
       "4  What kind of routines do general gymnastic gro...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['promoted Communist revolutions'], '...  \n",
       "1  {'text': ['a Provincial Minister (Politician) ...  \n",
       "2       {'text': ['a rally'], 'answer_start': [840]}  \n",
       "3  {'text': ['Blue Ivy Carter'], 'answer_start': ...  \n",
       "4  {'text': ['synchronized, choreographed routine...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Для удобной визуализации можно применить pandas\n",
    "datframe_squad = dataset_squad[\"train\"].to_pandas()\n",
    "datframe_squad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18a69f",
   "metadata": {},
   "source": [
    "# Токенизация данных\n",
    "BETR это популярная модель, но существуют более легкие и быстрые модели с такой же точностью и лучше использовать их\n",
    "https://youtu.be/u--UVvH-LIQ?t=563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79b64b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "modelcheckpoint = \"microsoft/miniLM-L12-H384-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelcheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c438c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"After the formation of the People's Republic of China in 1949, the Chinese government named the Western nations, led by the United States, as the biggest threat to its national security. Basing this judgment on China's century of humiliation beginning in the early 19th century, American support for the Nationalists during the Chinese Civil War, and the ideological struggles between revolutionaries and reactionaries, the Chinese leadership believed that China would become a critical battleground in the United States' crusade against Communism. As a countermeasure and to elevate China's standing among the worldwide Communist movements, the Chinese leadership adopted a foreign policy that actively promoted Communist revolutions throughout territories on China's periphery.\"]\n",
      "{'input_ids': [[101, 2044, 1996, 4195, 1997, 1996, 2111, 1005, 1055, 3072, 1997, 2859, 1999, 4085, 1010, 1996, 2822, 2231, 2315, 1996, 2530, 3741, 1010, 2419, 2011, 1996, 2142, 2163, 1010, 2004, 1996, 5221, 5081, 2000, 2049, 2120, 3036, 1012, 6403, 2290, 2023, 8689, 2006, 2859, 1005, 1055, 2301, 1997, 21171, 2927, 1999, 1996, 2220, 3708, 2301, 1010, 2137, 2490, 2005, 1996, 17934, 2076, 1996, 2822, 2942, 2162, 1010, 1998, 1996, 17859, 11785, 2090, 24517, 1998, 4668, 12086, 1010, 1996, 2822, 4105, 3373, 2008, 2859, 2052, 2468, 1037, 4187, 2645, 16365, 1999, 1996, 2142, 2163, 1005, 16282, 2114, 15523, 1012, 2004, 1037, 4675, 4168, 3022, 5397, 1998, 2000, 3449, 13331, 2618, 2859, 1005, 1055, 3061, 2426, 1996, 4969, 4750, 5750, 1010, 1996, 2822, 4105, 4233, 1037, 3097, 3343, 2008, 8851, 3755, 4750, 25239, 2802, 6500, 2006, 2859, 1005, 1055, 23275, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Пример: данные до токенизации\n",
    "test1 = dataset_squad[\"train\"][\"context\"][:1]\n",
    "print(test1)\n",
    "# Пример: токенизированные данные\n",
    "test2 = tokenizer(dataset_squad[\"train\"][\"context\"][:1])\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6420829b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '573173d8497a881900248f0c',\n",
       " 'title': 'Egypt',\n",
       " 'context': 'The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.',\n",
       " 'question': 'What percentage of Egyptians polled support death penalty for those leaving Islam?',\n",
       " 'answers': {'text': ['84%'], 'answer_start': [468]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# До обработки\n",
    "dataset_squad['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dabd64ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 78839/78839 [00:21<00:00, 3678.79 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 79675\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id', 'start_positions', 'end_positions'],\n",
      "        num_rows: 10784\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id', 'start_positions', 'end_positions'],\n",
      "        num_rows: 8849\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Функция для токенизации обучающих данных\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_map[i]\n",
    "        answer = answers[sample_index]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_start = next((j for j, sid in enumerate(sequence_ids) if sid == 1), None)\n",
    "        context_end = next((j for j, sid in reversed(list(enumerate(sequence_ids))) if sid == 1), None)\n",
    "\n",
    "        if context_start is None:\n",
    "            tokenized_examples[\"start_positions\"].append(0)\n",
    "            tokenized_examples[\"end_positions\"].append(0)\n",
    "            continue\n",
    "\n",
    "        if offsets[context_start][0] > start_char or offsets[context_end][1] < end_char:\n",
    "            # Ответ вне текущего токенизированного сплита контекста\n",
    "            tokenized_examples[\"start_positions\"].append(0)\n",
    "            tokenized_examples[\"end_positions\"].append(0)\n",
    "        else:\n",
    "            # Находим стартовый токен\n",
    "            start_token_idx = context_start\n",
    "            while start_token_idx < len(offsets) and offsets[start_token_idx][0] <= start_char:\n",
    "                start_token_idx += 1\n",
    "            tokenized_examples[\"start_positions\"].append(start_token_idx - 1)\n",
    "\n",
    "            # Находим конечный токен\n",
    "            end_token_idx = context_end\n",
    "            while end_token_idx >= context_start and offsets[end_token_idx][1] >= end_char:\n",
    "                end_token_idx -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(end_token_idx + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "def preprocess_evaluation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "    # Получаем answers безопасно, так как в test сплите их может не быть\n",
    "    answers = examples.get(\"answers\", None)\n",
    "\n",
    "    # Токенизация (return_offsets_mapping=True - обязательно)\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Сохраняем ID исходного примера\n",
    "    sample_map = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "    tokenized_examples[\"start_positions\"] = [] # Нужно для Validation Loss!\n",
    "    tokenized_examples[\"end_positions\"] = []   # Нужно для Validation Loss!\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sample_index = sample_map[i]\n",
    "        \n",
    "        # 1. Сохраняем example_id\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # 2. Обнуляем смещения для токенов вопроса (как вы делали раньше - для постобработки)\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        offset_mapping[i] = [\n",
    "            (o if sequence_ids[j] == 1 else None)\n",
    "            for j, o in enumerate(offset_mapping[i])\n",
    "        ]\n",
    "        \n",
    "        # 3. Добавляем метки (start/end positions) для Validation Loss\n",
    "        if answers and answers[sample_index].get(\"answer_start\"):\n",
    "            answer = answers[sample_index]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "            context_start = next((j for j, sid in enumerate(sequence_ids) if sid == 1), None)\n",
    "            context_end = next((j for j, sid in reversed(list(enumerate(sequence_ids))) if sid == 1), None)\n",
    "\n",
    "            # Находим позиции токенов (логика из функции обучения)\n",
    "            if context_start is None or offset_mapping[i][context_start] is None:\n",
    "                 start_pos, end_pos = 0, 0\n",
    "            elif offset_mapping[i][context_start][0] > start_char or offset_mapping[i][context_end][1] < end_char:\n",
    "                start_pos, end_pos = 0, 0\n",
    "            else:\n",
    "                start_token_idx = context_start\n",
    "                while start_token_idx < len(offset_mapping[i]) and offset_mapping[i][start_token_idx] is not None and offset_mapping[i][start_token_idx][0] <= start_char:\n",
    "                    start_token_idx += 1\n",
    "                start_pos = start_token_idx - 1\n",
    "\n",
    "                end_token_idx = context_end\n",
    "                while end_token_idx >= context_start and offset_mapping[i][end_token_idx] is not None and offset_mapping[i][end_token_idx][1] >= end_char:\n",
    "                    end_token_idx -= 1\n",
    "                end_pos = end_token_idx + 1\n",
    "\n",
    "            tokenized_examples[\"start_positions\"].append(start_pos)\n",
    "            tokenized_examples[\"end_positions\"].append(end_pos)\n",
    "        else:\n",
    "            # Для примеров без ответа или тестового набора\n",
    "            tokenized_examples[\"start_positions\"].append(0)\n",
    "            tokenized_examples[\"end_positions\"].append(0)\n",
    "            \n",
    "    return tokenized_examples\n",
    "\n",
    "# Токенизация обучающего сплита\n",
    "tokenized_train = dataset_squad[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_squad[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Токенизация валидационного и тестового сплитов\n",
    "tokenized_val = dataset_squad[\"validation\"].map(\n",
    "    preprocess_evaluation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_squad[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "tokenized_test = dataset_squad[\"test\"].map(\n",
    "    preprocess_evaluation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_squad[\"test\"].column_names\n",
    ")\n",
    "\n",
    "tokenized_dataset = DatasetDict({\n",
    "    \"train\": tokenized_train,\n",
    "    \"validation\": tokenized_val,\n",
    "    \"test\": tokenized_test\n",
    "})\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af923f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2000,\n",
       "  2265,\n",
       "  2037,\n",
       "  3997,\n",
       "  1999,\n",
       "  1996,\n",
       "  2248,\n",
       "  4750,\n",
       "  2929,\n",
       "  1010,\n",
       "  2054,\n",
       "  2106,\n",
       "  2859,\n",
       "  2079,\n",
       "  1029,\n",
       "  102,\n",
       "  2044,\n",
       "  1996,\n",
       "  4195,\n",
       "  1997,\n",
       "  1996,\n",
       "  2111,\n",
       "  1005,\n",
       "  1055,\n",
       "  3072,\n",
       "  1997,\n",
       "  2859,\n",
       "  1999,\n",
       "  4085,\n",
       "  1010,\n",
       "  1996,\n",
       "  2822,\n",
       "  2231,\n",
       "  2315,\n",
       "  1996,\n",
       "  2530,\n",
       "  3741,\n",
       "  1010,\n",
       "  2419,\n",
       "  2011,\n",
       "  1996,\n",
       "  2142,\n",
       "  2163,\n",
       "  1010,\n",
       "  2004,\n",
       "  1996,\n",
       "  5221,\n",
       "  5081,\n",
       "  2000,\n",
       "  2049,\n",
       "  2120,\n",
       "  3036,\n",
       "  1012,\n",
       "  6403,\n",
       "  2290,\n",
       "  2023,\n",
       "  8689,\n",
       "  2006,\n",
       "  2859,\n",
       "  1005,\n",
       "  1055,\n",
       "  2301,\n",
       "  1997,\n",
       "  21171,\n",
       "  2927,\n",
       "  1999,\n",
       "  1996,\n",
       "  2220,\n",
       "  3708,\n",
       "  2301,\n",
       "  1010,\n",
       "  2137,\n",
       "  2490,\n",
       "  2005,\n",
       "  1996,\n",
       "  17934,\n",
       "  2076,\n",
       "  1996,\n",
       "  2822,\n",
       "  2942,\n",
       "  2162,\n",
       "  1010,\n",
       "  1998,\n",
       "  1996,\n",
       "  17859,\n",
       "  11785,\n",
       "  2090,\n",
       "  24517,\n",
       "  1998,\n",
       "  4668,\n",
       "  12086,\n",
       "  1010,\n",
       "  1996,\n",
       "  2822,\n",
       "  4105,\n",
       "  3373,\n",
       "  2008,\n",
       "  2859,\n",
       "  2052,\n",
       "  2468,\n",
       "  1037,\n",
       "  4187,\n",
       "  2645,\n",
       "  16365,\n",
       "  1999,\n",
       "  1996,\n",
       "  2142,\n",
       "  2163,\n",
       "  1005,\n",
       "  16282,\n",
       "  2114,\n",
       "  15523,\n",
       "  1012,\n",
       "  2004,\n",
       "  1037,\n",
       "  4675,\n",
       "  4168,\n",
       "  3022,\n",
       "  5397,\n",
       "  1998,\n",
       "  2000,\n",
       "  3449,\n",
       "  13331,\n",
       "  2618,\n",
       "  2859,\n",
       "  1005,\n",
       "  1055,\n",
       "  3061,\n",
       "  2426,\n",
       "  1996,\n",
       "  4969,\n",
       "  4750,\n",
       "  5750,\n",
       "  1010,\n",
       "  1996,\n",
       "  2822,\n",
       "  4105,\n",
       "  4233,\n",
       "  1037,\n",
       "  3097,\n",
       "  3343,\n",
       "  2008,\n",
       "  8851,\n",
       "  3755,\n",
       "  4750,\n",
       "  25239,\n",
       "  2802,\n",
       "  6500,\n",
       "  2006,\n",
       "  2859,\n",
       "  1005,\n",
       "  1055,\n",
       "  23275,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'start_positions': 144,\n",
       " 'end_positions': 146}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# После обработки\n",
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# После обработки\n",
    "tokenized_dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb22f45",
   "metadata": {},
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bd71d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для метрик\n",
    "import collections \n",
    "import numpy as np\n",
    "import evaluate # Загрузка метрики\n",
    "\n",
    "# Функция постобработки\n",
    "# Эта функция сопоставляет логиты с исходными текстами, выбирает лучший ответ и преобразует его в текст.\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    \n",
    "    # Сопоставляем токенизированные \"features\" (сплиты) с оригинальными \"examples\"\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # Инициализация словаря для хранения лучших ответов\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        min_null_score = None # Для SQuAD v2, но полезно для отслеживания\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        \n",
    "        # Перебор всех токенизированных сплитов, относящихся к этому примеру\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Выбираем N лучших стартовых и конечных логитов\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            \n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 1. Фильтруем: end должна быть позже start\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    # 2. Фильтруем: длина ответа не должна превышать max_answer_length\n",
    "                    if end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    \n",
    "                    # 3. Фильтруем: убеждаемся, что мы в пределах контекста (не в CLS, SEP, QUESTION)\n",
    "                    # sequence_ids уже был обнулен для вопроса в pre-processing, \n",
    "                    # поэтому проверяем, что смещения не None\n",
    "                    \n",
    "                    start_char_span = offset_mapping[start_index]\n",
    "                    end_char_span = offset_mapping[end_index]\n",
    "                    \n",
    "                    if start_char_span is None or end_char_span is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Извлекаем текстовый ответ, используя смещения\n",
    "                    start_char = start_char_span[0]\n",
    "                    end_char = end_char_span[1]\n",
    "                    answer_text = context[start_char:end_char]\n",
    "                    \n",
    "                    # Добавляем валидный ответ\n",
    "                    valid_answers.append({\n",
    "                        \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                        \"text\": answer_text,\n",
    "                        \"start_logit\": start_logits[start_index],\n",
    "                        \"end_logit\": end_logits[end_index],\n",
    "                    })\n",
    "\n",
    "        # Выбираем лучший ответ\n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            # Если не найдено валидного ответа, возвращаем пустую строку\n",
    "            predictions[example[\"id\"]] = \"\"\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Сохраняем исходные наборы данных (нужны для постобработки)\n",
    "validation_features = tokenized_dataset[\"validation\"]\n",
    "validation_set = dataset_squad[\"validation\"] \n",
    "metric = evaluate.load(\"squad\") \n",
    "\n",
    "import traceback\n",
    "\n",
    "# Фабричная функция для compute_metrics (включает try/except для надежности)\n",
    "def make_qa_compute_metrics(raw_examples, tokenized_features, squad_metric):\n",
    "    \n",
    "    def compute_qa_metrics(p):\n",
    "        try:\n",
    "            start_logits, end_logits = p.predictions\n",
    "            \n",
    "            predictions = postprocess_qa_predictions(\n",
    "                examples=raw_examples,\n",
    "                features=tokenized_features,\n",
    "                raw_predictions=(start_logits, end_logits),\n",
    "                n_best_size=20,\n",
    "                max_answer_length=30\n",
    "            )\n",
    "            \n",
    "            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "            references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in raw_examples]\n",
    "            \n",
    "            return squad_metric.compute(predictions=formatted_predictions, references=references)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Выводит ошибку, если постобработка сбоит\n",
    "            print(f\"\\n!!! ОШИБКА ВЫЧИСЛЕНИЯ МЕТРИК: {e} !!!\")\n",
    "            return {\"f1\": 0.0, \"exact_match\": 0.0}\n",
    "\n",
    "    return compute_qa_metrics\n",
    "\n",
    "# Создаем функцию, которую будем передавать в Trainer\n",
    "qa_compute_metrics_fn = make_qa_compute_metrics(\n",
    "    validation_set, \n",
    "    validation_features, \n",
    "    metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852e041",
   "metadata": {},
   "source": [
    "# Сбор всех данных воедино"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "909badab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/miniLM-L12-H384-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Подготовка модели и зависимостей\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer  # рассматриваем как задачу классификации последовательностей\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(modelcheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3778844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Определение аргументов обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"local_models/minilm_squad\",  # Директория для сохранения результатов\n",
    "    eval_strategy=\"epoch\",          # Оценивать после каждой эпохи\n",
    "    learning_rate=2e-5,             # Скорость обучения\n",
    "    per_device_train_batch_size=16, # Размер батча для обучения\n",
    "    per_device_eval_batch_size=16,  # Размер батча для валидации\n",
    "    num_train_epochs=3,             # Количество эпох\n",
    "    weight_decay=0.01,              # Регуляризация\n",
    "    push_to_hub=False,              # Не загружать на Hugging Face Hub\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37c11bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_31768\\194113636.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 3. Инициализация\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=qa_compute_metrics_fn,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer # Передаем токенизатор, чтобы Trainer мог его сохранить\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f91ed012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14940' max='14940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14940/14940 48:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.014200</td>\n",
       "      <td>0.969763</td>\n",
       "      <td>81.021760</td>\n",
       "      <td>88.863876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.926080</td>\n",
       "      <td>83.150426</td>\n",
       "      <td>90.226364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.716500</td>\n",
       "      <td>0.931661</td>\n",
       "      <td>83.472091</td>\n",
       "      <td>90.456945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14940, training_loss=0.9447985811246288, metrics={'train_runtime': 2902.5403, 'train_samples_per_second': 82.35, 'train_steps_per_second': 5.147, 'total_flos': 1.17275044876416e+16, 'train_loss': 0.9447985811246288, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Запуск обучения https://youtu.be/u--UVvH-LIQ?t=1068\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d03178",
   "metadata": {},
   "source": [
    "| Метрика | Значение | Интерпретация |\n",
    "| --- | --- | --- |\n",
    "| **Training Loss** | **1.0474** | Уровень ошибки модели на обучающем наборе данных. Это число будет снижаться с каждой эпохой. |\n",
    "| **Validation Loss** | **0.993388** | Уровень ошибки модели на **валидационном** наборе данных. Тот факт, что оно **меньше**, чем `Training Loss`, указывает на то, что модель **не переобучается** (пока) и хорошо обобщает данные. |\n",
    "| **Exact Match (EM)** | **81.24%** | В **81.24%** случаев предсказанный ответ **абсолютно точно** совпал с одним из правильных ответов золотого стандарта. |\n",
    "| **F1 Score** | **88.77%** | Среднее гармоническое между точностью и полнотой совпадения слов. Это высокий показатель, говорящий о том, что модель очень хорошо находит правильные границы ответа в контексте. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f43a09",
   "metadata": {},
   "source": [
    "# Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf733528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('local_models/minilm_squad\\\\tokenizer_config.json',\n",
       " 'local_models/minilm_squad\\\\special_tokens_map.json',\n",
       " 'local_models/minilm_squad\\\\vocab.txt',\n",
       " 'local_models/minilm_squad\\\\added_tokens.json',\n",
       " 'local_models/minilm_squad\\\\tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"local_models/minilm_squad\")\n",
    "tokenizer.save_pretrained(\"local_models/minilm_squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e607bb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(tokenized_dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684bab1",
   "metadata": {},
   "source": [
    "# Использование обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef501ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/miniLM-L12-H384-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст: Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\n",
      "Вопрос: Near which river is the city located?\n",
      "---\n",
      "Ответ модели: formerly known as Nur-Sultan\n",
      "Оценка уверенности (Score): 0.0007\n",
      "Позиция ответа (Start/End): 8-36\n"
     ]
    }
   ],
   "source": [
    "# Необученная модель\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "qa_orig_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=modelcheckpoint,\n",
    "    tokenizer=modelcheckpoint\n",
    ")\n",
    "\n",
    "# Пример контекста из вашего датасета SQuAD\n",
    "context_text = (\n",
    "    \"Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.\"\n",
    "    \"The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\"\n",
    ")\n",
    "\n",
    "question_text = \"Near which river is the city located?\"\n",
    "\n",
    "# Выполняем инференс\n",
    "result = qa_orig_pipeline(\n",
    "    question=question_text,\n",
    "    context=context_text\n",
    ")\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Контекст: {context_text}\")\n",
    "print(f\"Вопрос: {question_text}\")\n",
    "print(\"---\")\n",
    "print(f\"Ответ модели: {result['answer']}\")\n",
    "print(f\"Оценка уверенности (Score): {result['score']:.4f}\")\n",
    "print(f\"Позиция ответа (Start/End): {result['start']}-{result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8da52e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст: Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\n",
      "Вопрос: Since what year has Astana been the capital of Kazakhstan?\n",
      "---\n",
      "Ответ модели: 1997\n",
      "Оценка уверенности (Score): 0.9829\n",
      "Позиция ответа (Start/End): 79-83\n"
     ]
    }
   ],
   "source": [
    "# Обученная модель\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "modelcheckpoint_new = \"local_models/minilm_squad\"\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=modelcheckpoint_new,\n",
    "    tokenizer=modelcheckpoint_new\n",
    ")\n",
    "\n",
    "# Пример контекста из вашего датасета SQuAD\n",
    "context_text = (\n",
    "    \"Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.\"\n",
    "    \"The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\"\n",
    ")\n",
    "\n",
    "question_text = \"Since what year has Astana been the capital of Kazakhstan?\"\n",
    "\n",
    "# Выполняем инференс\n",
    "result = qa_pipeline(\n",
    "    question=question_text,\n",
    "    context=context_text\n",
    ")\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Контекст: {context_text}\")\n",
    "print(f\"Вопрос: {question_text}\")\n",
    "print(\"---\")\n",
    "print(f\"Ответ модели: {result['answer']}\")\n",
    "print(f\"Оценка уверенности (Score): {result['score']:.4f}\")\n",
    "print(f\"Позиция ответа (Start/End): {result['start']}-{result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f397a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст: Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\n",
      "Вопрос: Near which river is the city located?\n",
      "---\n",
      "Ответ модели: Ishim River\n",
      "Оценка уверенности (Score): 0.7141\n",
      "Позиция ответа (Start/End): 124-135\n"
     ]
    }
   ],
   "source": [
    "# Обученная модель\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "modelcheckpoint_new = \"local_models/minilm_squad\"\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=modelcheckpoint_new,\n",
    "    tokenizer=modelcheckpoint_new\n",
    ")\n",
    "\n",
    "# Пример контекста из вашего датасета SQuAD\n",
    "context_text = (\n",
    "    \"Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.\"\n",
    "    \"The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\"\n",
    ")\n",
    "\n",
    "question_text = \"Near which river is the city located?\"\n",
    "\n",
    "# Выполняем инференс\n",
    "result = qa_pipeline(\n",
    "    question=question_text,\n",
    "    context=context_text\n",
    ")\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Контекст: {context_text}\")\n",
    "print(f\"Вопрос: {question_text}\")\n",
    "print(\"---\")\n",
    "print(f\"Ответ модели: {result['answer']}\")\n",
    "print(f\"Оценка уверенности (Score): {result['score']:.4f}\")\n",
    "print(f\"Позиция ответа (Start/End): {result['start']}-{result['end']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
