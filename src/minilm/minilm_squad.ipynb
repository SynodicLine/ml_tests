{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac93753c",
   "metadata": {},
   "source": [
    "https://youtu.be/u--UVvH-LIQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafce58",
   "metadata": {},
   "source": [
    "# Выбираем пакет данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699a6a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\hf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Набор данных Вопросы-Ответы\n",
    "# Вопросы-Ответы (Question Answering)\n",
    "# Задача для практики с моделями, которые должны найти ответ в контексте (extractive QA).\n",
    "dataset_squad = load_dataset(\"squad\")\n",
    "dataset_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2baf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f450d01",
   "metadata": {},
   "source": [
    "Разбиваю чать Train на Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf06e7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 78839\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 8760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Извлекаем существующий спилит\n",
    "train_data = dataset_squad[\"train\"]\n",
    "\n",
    "# 2. Разбиваем на два сплита\n",
    "# test_size=0.1\n",
    "# seeseed=42 для воспроизводимости\n",
    "test_split = train_data.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# 3. Обновляем исходный объект DatasetDict\n",
    "dataset_squad[\"train\"] = test_split[\"train\"]\n",
    "dataset_squad[\"test\"] = test_split[\"test\"]\n",
    "\n",
    "dataset_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00979e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57263127ec44d21400f3dbf9</td>\n",
       "      <td>Korean_War</td>\n",
       "      <td>After the formation of the People's Republic o...</td>\n",
       "      <td>To show their strength in the international Co...</td>\n",
       "      <td>{'text': ['promoted Communist revolutions'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733f93e4776f41900661602</td>\n",
       "      <td>Punjab,_Pakistan</td>\n",
       "      <td>There are 48 departments in Punjab government....</td>\n",
       "      <td>Who heads each government department?</td>\n",
       "      <td>{'text': ['a Provincial Minister (Politician) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56bfae97a10cfb1400551236</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé and husband Jay Z are friends with Pre...</td>\n",
       "      <td>What did they attend in July 2013?</td>\n",
       "      <td>{'text': ['a rally'], 'answer_start': [840]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bfa087a10cfb14005511da</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>On January 7, 2012, Beyoncé gave birth to her ...</td>\n",
       "      <td>What was the child's name?</td>\n",
       "      <td>{'text': ['Blue Ivy Carter'], 'answer_start': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570ceadbfed7b91900d45ad5</td>\n",
       "      <td>Gymnastics</td>\n",
       "      <td>General gymnastics enables people of all ages ...</td>\n",
       "      <td>What kind of routines do general gymnastic gro...</td>\n",
       "      <td>{'text': ['synchronized, choreographed routine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id             title  \\\n",
       "0  57263127ec44d21400f3dbf9        Korean_War   \n",
       "1  5733f93e4776f41900661602  Punjab,_Pakistan   \n",
       "2  56bfae97a10cfb1400551236           Beyoncé   \n",
       "3  56bfa087a10cfb14005511da           Beyoncé   \n",
       "4  570ceadbfed7b91900d45ad5        Gymnastics   \n",
       "\n",
       "                                             context  \\\n",
       "0  After the formation of the People's Republic o...   \n",
       "1  There are 48 departments in Punjab government....   \n",
       "2  Beyoncé and husband Jay Z are friends with Pre...   \n",
       "3  On January 7, 2012, Beyoncé gave birth to her ...   \n",
       "4  General gymnastics enables people of all ages ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To show their strength in the international Co...   \n",
       "1              Who heads each government department?   \n",
       "2                 What did they attend in July 2013?   \n",
       "3                         What was the child's name?   \n",
       "4  What kind of routines do general gymnastic gro...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['promoted Communist revolutions'], '...  \n",
       "1  {'text': ['a Provincial Minister (Politician) ...  \n",
       "2       {'text': ['a rally'], 'answer_start': [840]}  \n",
       "3  {'text': ['Blue Ivy Carter'], 'answer_start': ...  \n",
       "4  {'text': ['synchronized, choreographed routine...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Для удобной визуализации можно применить pandas\n",
    "datframe_squad = dataset_squad[\"train\"].to_pandas()\n",
    "datframe_squad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18a69f",
   "metadata": {},
   "source": [
    "# Токенизация данных\n",
    "BETR это популярная модель, но существуют более легкие и быстрые модели с такой же точностью и лучше использовать их\n",
    "https://youtu.be/u--UVvH-LIQ?t=563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b64b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "modelcheckpoint = \"microsoft/miniLM-L12-H384-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelcheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c438c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"After the formation of the People's Republic of China in 1949, the Chinese government named the Western nations, led by the United States, as the biggest threat to its national security. Basing this judgment on China's century of humiliation beginning in the early 19th century, American support for the Nationalists during the Chinese Civil War, and the ideological struggles between revolutionaries and reactionaries, the Chinese leadership believed that China would become a critical battleground in the United States' crusade against Communism. As a countermeasure and to elevate China's standing among the worldwide Communist movements, the Chinese leadership adopted a foreign policy that actively promoted Communist revolutions throughout territories on China's periphery.\"]\n",
      "{'input_ids': [[101, 2044, 1996, 4195, 1997, 1996, 2111, 1005, 1055, 3072, 1997, 2859, 1999, 4085, 1010, 1996, 2822, 2231, 2315, 1996, 2530, 3741, 1010, 2419, 2011, 1996, 2142, 2163, 1010, 2004, 1996, 5221, 5081, 2000, 2049, 2120, 3036, 1012, 6403, 2290, 2023, 8689, 2006, 2859, 1005, 1055, 2301, 1997, 21171, 2927, 1999, 1996, 2220, 3708, 2301, 1010, 2137, 2490, 2005, 1996, 17934, 2076, 1996, 2822, 2942, 2162, 1010, 1998, 1996, 17859, 11785, 2090, 24517, 1998, 4668, 12086, 1010, 1996, 2822, 4105, 3373, 2008, 2859, 2052, 2468, 1037, 4187, 2645, 16365, 1999, 1996, 2142, 2163, 1005, 16282, 2114, 15523, 1012, 2004, 1037, 4675, 4168, 3022, 5397, 1998, 2000, 3449, 13331, 2618, 2859, 1005, 1055, 3061, 2426, 1996, 4969, 4750, 5750, 1010, 1996, 2822, 4105, 4233, 1037, 3097, 3343, 2008, 8851, 3755, 4750, 25239, 2802, 6500, 2006, 2859, 1005, 1055, 23275, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Пример: данные до токенизации\n",
    "test1 = dataset_squad[\"train\"][\"context\"][:1]\n",
    "print(test1)\n",
    "# Пример: токенизированные данные\n",
    "test2 = tokenizer(dataset_squad[\"train\"][\"context\"][:1])\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6420829b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '573173d8497a881900248f0c',\n",
       " 'title': 'Egypt',\n",
       " 'context': 'The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.',\n",
       " 'question': 'What percentage of Egyptians polled support death penalty for those leaving Islam?',\n",
       " 'answers': {'text': ['84%'], 'answer_start': [468]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# До обработки\n",
    "dataset_squad['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabd64ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10570/10570 [00:03<00:00, 3089.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 79675\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "        num_rows: 10784\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "        num_rows: 8849\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Функция для токенизации обучающих данных\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    # 1. Токенизация вопроса и контекста\n",
    "    # truncation=\"only_second\": обрезает только контекст, если он слишком длинный\n",
    "    # stride=128: позволяет окнам контекста перекрываться (важно для длинных текстов)\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,  # Устанавливаем max_length для MiniLM\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True, # Важно для сопоставления с исходными примерами\n",
    "        return_offsets_mapping=True,    # Возвращает смещения для поиска ответа\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 2. Сопоставление новых примеров (сплитов) с оригинальными\n",
    "    sample_map = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 3. Нахождение позиций ответа (start_position, end_position)\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_map[i]\n",
    "        answer = answers[sample_index]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        # Последовательность токенов, соответствующая вопросу (0) или контексту (1)\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # Найти начало и конец контекста в токенах\n",
    "        context_start = next((j for j, sid in enumerate(sequence_ids) if sid == 1), None)\n",
    "        context_end = next((j for j, sid in reversed(list(enumerate(sequence_ids))) if sid == 1), None)\n",
    "\n",
    "        if context_start is None:\n",
    "            # Если контекст обрезан до нуля, ответ не найти\n",
    "            tokenized_examples[\"start_positions\"].append(0)\n",
    "            tokenized_examples[\"end_positions\"].append(0)\n",
    "            continue\n",
    "\n",
    "        # Найти начальную позицию ответа\n",
    "        if offsets[context_start][0] > start_char or offsets[context_end][1] < end_char:\n",
    "            # Ответ вне текущего токенизированного сплита контекста\n",
    "            tokenized_examples[\"start_positions\"].append(0)\n",
    "            tokenized_examples[\"end_positions\"].append(0)\n",
    "        else:\n",
    "            # Ответ находится в сплите: ищем стартовый токен\n",
    "            start_token_idx = context_start\n",
    "            while start_token_idx < len(offsets) and offsets[start_token_idx][0] <= start_char:\n",
    "                start_token_idx += 1\n",
    "            tokenized_examples[\"start_positions\"].append(start_token_idx - 1)\n",
    "\n",
    "            # Находим конечную позицию ответа\n",
    "            end_token_idx = context_end\n",
    "            while end_token_idx >= context_start and offsets[end_token_idx][1] >= end_char:\n",
    "                end_token_idx -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(end_token_idx + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "\n",
    "    # Токенизация, аналогичная обучающим данным\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Сохраняем ID исходного примера для каждого нового сплита (важно для оценки)\n",
    "    sample_map = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sample_index = sample_map[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Обнуляем смещения для токенов вопроса, чтобы не путаться при оценке\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        offset_mapping = tokenized_examples[\"offset_mapping\"][i]\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[j] == 1 else None)\n",
    "            for j, o in enumerate(offset_mapping)\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# Токенизация обучающего сплита\n",
    "tokenized_train = dataset_squad[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_squad[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Токенизация валидационного и тестового сплитов\n",
    "tokenized_val = dataset_squad[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_squad[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "tokenized_test = dataset_squad[\"test\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_squad[\"test\"].column_names\n",
    ")\n",
    "\n",
    "# Объединяем обратно в DatasetDict (по желанию)\n",
    "tokenized_dataset = DatasetDict({\n",
    "    \"train\": tokenized_train,\n",
    "    \"validation\": tokenized_val,\n",
    "    \"test\": tokenized_test\n",
    "})\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af923f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2000,\n",
       "  2265,\n",
       "  2037,\n",
       "  3997,\n",
       "  1999,\n",
       "  1996,\n",
       "  2248,\n",
       "  4750,\n",
       "  2929,\n",
       "  1010,\n",
       "  2054,\n",
       "  2106,\n",
       "  2859,\n",
       "  2079,\n",
       "  1029,\n",
       "  102,\n",
       "  2044,\n",
       "  1996,\n",
       "  4195,\n",
       "  1997,\n",
       "  1996,\n",
       "  2111,\n",
       "  1005,\n",
       "  1055,\n",
       "  3072,\n",
       "  1997,\n",
       "  2859,\n",
       "  1999,\n",
       "  4085,\n",
       "  1010,\n",
       "  1996,\n",
       "  2822,\n",
       "  2231,\n",
       "  2315,\n",
       "  1996,\n",
       "  2530,\n",
       "  3741,\n",
       "  1010,\n",
       "  2419,\n",
       "  2011,\n",
       "  1996,\n",
       "  2142,\n",
       "  2163,\n",
       "  1010,\n",
       "  2004,\n",
       "  1996,\n",
       "  5221,\n",
       "  5081,\n",
       "  2000,\n",
       "  2049,\n",
       "  2120,\n",
       "  3036,\n",
       "  1012,\n",
       "  6403,\n",
       "  2290,\n",
       "  2023,\n",
       "  8689,\n",
       "  2006,\n",
       "  2859,\n",
       "  1005,\n",
       "  1055,\n",
       "  2301,\n",
       "  1997,\n",
       "  21171,\n",
       "  2927,\n",
       "  1999,\n",
       "  1996,\n",
       "  2220,\n",
       "  3708,\n",
       "  2301,\n",
       "  1010,\n",
       "  2137,\n",
       "  2490,\n",
       "  2005,\n",
       "  1996,\n",
       "  17934,\n",
       "  2076,\n",
       "  1996,\n",
       "  2822,\n",
       "  2942,\n",
       "  2162,\n",
       "  1010,\n",
       "  1998,\n",
       "  1996,\n",
       "  17859,\n",
       "  11785,\n",
       "  2090,\n",
       "  24517,\n",
       "  1998,\n",
       "  4668,\n",
       "  12086,\n",
       "  1010,\n",
       "  1996,\n",
       "  2822,\n",
       "  4105,\n",
       "  3373,\n",
       "  2008,\n",
       "  2859,\n",
       "  2052,\n",
       "  2468,\n",
       "  1037,\n",
       "  4187,\n",
       "  2645,\n",
       "  16365,\n",
       "  1999,\n",
       "  1996,\n",
       "  2142,\n",
       "  2163,\n",
       "  1005,\n",
       "  16282,\n",
       "  2114,\n",
       "  15523,\n",
       "  1012,\n",
       "  2004,\n",
       "  1037,\n",
       "  4675,\n",
       "  4168,\n",
       "  3022,\n",
       "  5397,\n",
       "  1998,\n",
       "  2000,\n",
       "  3449,\n",
       "  13331,\n",
       "  2618,\n",
       "  2859,\n",
       "  1005,\n",
       "  1055,\n",
       "  3061,\n",
       "  2426,\n",
       "  1996,\n",
       "  4969,\n",
       "  4750,\n",
       "  5750,\n",
       "  1010,\n",
       "  1996,\n",
       "  2822,\n",
       "  4105,\n",
       "  4233,\n",
       "  1037,\n",
       "  3097,\n",
       "  3343,\n",
       "  2008,\n",
       "  8851,\n",
       "  3755,\n",
       "  4750,\n",
       "  25239,\n",
       "  2802,\n",
       "  6500,\n",
       "  2006,\n",
       "  2859,\n",
       "  1005,\n",
       "  1055,\n",
       "  23275,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'start_positions': 144,\n",
       " 'end_positions': 146}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# После обработки\n",
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852e041",
   "metadata": {},
   "source": [
    "# Сбор всех данных воедино"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909badab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/miniLM-L12-H384-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Сбор всех данных воедино\n",
    "\n",
    "# 1. Подготовка модели и зависимостей\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer  # рассматриваем как задачу классификации последовательностей\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(modelcheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3778844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Определение аргументов обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"minilm_squad\",  # Директория для сохранения результатов\n",
    "    eval_strategy=\"epoch\",    # Оценивать после каждой эпохи\n",
    "    learning_rate=2e-5,             # Скорость обучения\n",
    "    per_device_train_batch_size=16, # Размер батча для обучения\n",
    "    per_device_eval_batch_size=16,  # Размер батча для валидации\n",
    "    num_train_epochs=3,             # Количество эпох\n",
    "    weight_decay=0.01,              # Регуляризация\n",
    "    push_to_hub=False,              # Не загружать на Hugging Face Hub\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c11bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10504\\2397557632.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 3. Инициализация\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer # Передаем токенизатор, чтобы Trainer мог его сохранить\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12fbec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем метрики\n",
    "# from sklearn.metrics import f1_score # f1_score - это среднее значение точности и полноты, это лучший способ учесть дисбаланс данных\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "#     return{\"f1\":f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfab9a6",
   "metadata": {},
   "source": [
    "# Запуск обучения\n",
    "https://youtu.be/u--UVvH-LIQ?t=1068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91ed012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14940' max='14940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14940/14940 34:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.025900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.718700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14940, training_loss=0.9610505804958114, metrics={'train_runtime': 2047.7969, 'train_samples_per_second': 116.723, 'train_steps_per_second': 7.296, 'total_flos': 1.17275044876416e+16, 'train_loss': 0.9610505804958114, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f43a09",
   "metadata": {},
   "source": [
    "# Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf733528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('minilm_squad\\\\tokenizer_config.json',\n",
       " 'minilm_squad\\\\special_tokens_map.json',\n",
       " 'minilm_squad\\\\vocab.txt',\n",
       " 'minilm_squad\\\\added_tokens.json',\n",
       " 'minilm_squad\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"local_models/minilm_squad\")\n",
    "tokenizer.save_pretrained(\"local_models/minilm_squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e607bb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(tokenized_dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684bab1",
   "metadata": {},
   "source": [
    "# Использование обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da52e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст: Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\n",
      "Вопрос: Since what year has Astana been the capital of Kazakhstan?\n",
      "---\n",
      "Ответ модели: 1997\n",
      "Оценка уверенности (Score): 0.9851\n",
      "Позиция ответа (Start/End): 79-83\n"
     ]
    }
   ],
   "source": [
    "# Обученная модель\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "modelcheckpoint_new = \"local_models/minilm_squad\"\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=modelcheckpoint_new,\n",
    "    tokenizer=modelcheckpoint_new\n",
    ")\n",
    "\n",
    "# Пример контекста из вашего датасета SQuAD\n",
    "context_text = (\n",
    "    \"Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.\"\n",
    "    \"The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\"\n",
    ")\n",
    "\n",
    "question_text = \"Since what year has Astana been the capital of Kazakhstan?\"\n",
    "\n",
    "# Выполняем инференс\n",
    "result = qa_pipeline(\n",
    "    question=question_text,\n",
    "    context=context_text\n",
    ")\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Контекст: {context_text}\")\n",
    "print(f\"Вопрос: {question_text}\")\n",
    "print(\"---\")\n",
    "print(f\"Ответ модели: {result['answer']}\")\n",
    "print(f\"Оценка уверенности (Score): {result['score']:.4f}\")\n",
    "print(f\"Позиция ответа (Start/End): {result['start']}-{result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f397a78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Обученная модель\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      5\u001b[39m modelcheckpoint_new = \u001b[33m\"\u001b[39m\u001b[33mlocal_models/minilm_squad\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m qa_pipeline = pipeline(\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion-answering\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     model=modelcheckpoint_new,\n\u001b[32m      9\u001b[39m     tokenizer=modelcheckpoint_new\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Обученная модель\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "modelcheckpoint_new = \"local_models/minilm_squad\"\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=modelcheckpoint_new,\n",
    "    tokenizer=modelcheckpoint_new\n",
    ")\n",
    "\n",
    "# Пример контекста из вашего датасета SQuAD\n",
    "context_text = (\n",
    "    \"Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.\"\n",
    "    \"The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\"\n",
    ")\n",
    "\n",
    "question_text = \"Near which river is the city located?\"\n",
    "\n",
    "# Выполняем инференс\n",
    "result = qa_pipeline(\n",
    "    question=question_text,\n",
    "    context=context_text\n",
    ")\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Контекст: {context_text}\")\n",
    "print(f\"Вопрос: {question_text}\")\n",
    "print(\"---\")\n",
    "print(f\"Ответ модели: {result['answer']}\")\n",
    "print(f\"Оценка уверенности (Score): {result['score']:.4f}\")\n",
    "print(f\"Позиция ответа (Start/End): {result['start']}-{result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef501ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/miniLM-L12-H384-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст: Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\n",
      "Вопрос: Near which river is the city located?\n",
      "---\n",
      "Ответ модели: Baiterek\n",
      "Оценка уверенности (Score): 0.0014\n",
      "Позиция ответа (Start/End): 196-204\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "# Необученная модель\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "qa_orig_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=modelcheckpoint,\n",
    "    tokenizer=modelcheckpoint\n",
    ")\n",
    "\n",
    "# Пример контекста из вашего датасета SQuAD\n",
    "context_text = (\n",
    "    \"Astana, formerly known as Nur-Sultan, has been the capital of Kazakhstan since 1997.\"\n",
    "    \"The city is located on the banks of the Ishim River and is known for its futuristic architecture, including the Baiterek Tower.\"\n",
    ")\n",
    "\n",
    "question_text = \"Near which river is the city located?\"\n",
    "\n",
    "# Выполняем инференс\n",
    "result = qa_orig_pipeline(\n",
    "    question=question_text,\n",
    "    context=context_text\n",
    ")\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Контекст: {context_text}\")\n",
    "print(f\"Вопрос: {question_text}\")\n",
    "print(\"---\")\n",
    "print(f\"Ответ модели: {result['answer']}\")\n",
    "print(f\"Оценка уверенности (Score): {result['score']:.4f}\")\n",
    "print(f\"Позиция ответа (Start/End): {result['start']}-{result['end']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
