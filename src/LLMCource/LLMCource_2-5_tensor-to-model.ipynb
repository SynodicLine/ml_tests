{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b6e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\ds-vl2\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "  2166  1012], shape=(14,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276213,  2.8789363]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tf.constant(ids)\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19258805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
      "   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e39d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS: ['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n",
      "IDS: [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "Input IDs: tf.Tensor(\n",
      "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]], shape=(1, 14), dtype=int32)\n",
      "Logits: tf.Tensor([[-2.7276213  2.8789363]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "# Последовательность действий\n",
    "# 1. Перобразовываем в токены\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print('TOKENS:' ,tokens)\n",
    "# 2. Получаем ID токенов\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('IDS:', ids)\n",
    "\n",
    "# 3. Этот шаг преобразует список числовых идентификаторов токенов (ids) в тензор TensorFlow\n",
    "input_ids = tf.constant([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "# 4. Передаем подготовленные данные в обученную модель для получения предсказания\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec0bd3",
   "metadata": {},
   "source": [
    "Батчинг - это отправка нескольких предложений через модель одновременно. Если у вас есть только одно предложение, вы можете просто создать батч с одной последовательностью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d13e659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tf.Tensor(\n",
      "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]\n",
      " [ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]], shape=(2, 14), dtype=int32)\n",
      "Logits: tf.Tensor(\n",
      "[[-2.7276204  2.8789377]\n",
      " [-2.7276204  2.8789377]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [ids, ids]\n",
    "\n",
    "input_ids = tf.constant(batched_ids)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f65d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_sequences: tf.Tensor(\n",
      "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
      "   2878  2166  1012   102]\n",
      " [  101  1045  5223  2023  2061  2172   999   102     0     0     0     0\n",
      "      0     0     0     0]], shape=(2, 16), dtype=int32)\n",
      "Logits: tf.Tensor(\n",
      "[[-1.5606993  1.6122842]\n",
      " [ 4.169231  -3.3464477]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# checkpoint — это имя конкретной модели\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# загружает токенизатор, который соответствует выбранной модели. Он знает, как преобразовать текст в последовательности чисел (токенов)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# загружает саму модель, которая будет классифицировать текст, работая в среде TensorFlow.\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1 = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "sequence2 = \"I hate this so much!\"\n",
    "\n",
    "#[sequence1, sequence2] — вы передаете обе строки в виде списка. Это позволяет токенизатору обрабатывать их вместе как один \"пакет\" (batch).\n",
    "# padding=True — этот аргумент указывает токенизатору, что он должен дополнить более короткую последовательность специальными токенами-заполнителями ([PAD]), \n",
    "# чтобы она имела ту же длину, что и самая длинная в пакете.\n",
    "# truncation=True - обрезание строки\n",
    "# return_tensors='tf' — этот аргумент сообщает токенизатору, что результат должен быть возвращен в виде тензоров TensorFlow, а не обычных списков Python.\n",
    "# В результате tokenizer_sequences становится словарем, содержащим тензор input_ids (идентификаторы токенов) и attention_mask (маску внимания).\n",
    "tokenizer_sequences = tokenizer([sequence1, sequence2], padding=True, return_tensors='tf')\n",
    "print(\"tokenizer_sequences:\", tokenizer_sequences['input_ids'])\n",
    "\n",
    "# Здесь вы передаете весь словарь tokenizer_sequences в модель. \n",
    "# Модель принимает его и автоматически использует как input_ids, так и attention_mask. \n",
    "# Маска внимания нужна для того, чтобы модель игнорировала токены-заполнители, которые были добавлены на предыдущем шаге.\n",
    "output = model(tokenizer_sequences)\n",
    "print(\"Logits:\", output.logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-vl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
