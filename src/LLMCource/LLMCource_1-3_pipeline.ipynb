{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0610a9f",
   "metadata": {},
   "source": [
    " # Источники\n",
    " https://huggingface.co/learn/llm-course/ru/chapter1/3?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e445e2",
   "metadata": {},
   "source": [
    "# pipeline в библиотеке transformers\n",
    "pipeline — это высокоуровневый, простой в использовании API от Hugging Face, предназначенный для решения различных задач с помощью моделей трансформеров. Он абстрагирует множество низкоуровневых деталей, таких как токенизация, загрузка модели и постобработка, позволяя быстро и эффективно использовать предобученные модели.\n",
    "\n",
    "Вот список наиболее распространённых вариантов использования pipeline с кратким описанием:\n",
    "\n",
    "1. text-classification (Классификация текста)\n",
    "Описание: Определяет категорию или метку для данного текста. Например, сентимент-анализ (положительный/отрицательный отзыв), классификация спама или определение темы текста.\n",
    "Пример: classifier = pipeline(\"text-classification\")\n",
    "\n",
    "2. zero-shot-classification (Классификация с \"нулевым выстрелом\")\n",
    "Описание: Позволяет классифицировать текст по меткам, которые модель не видела во время обучения. Вы предоставляете текст и список возможных меток, а модель оценивает, насколько текст соответствует каждой из них.\n",
    "Пример: classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "3. sentiment-analysis (Сентимент-анализ)\n",
    "Описание: Частный случай text-classification, который специально предназначен для определения эмоциональной окраски текста.\n",
    "\n",
    "4. fill-mask (Заполнение маски)\n",
    "Описание: Предсказывает слова, которые были \"замаскированы\" (скрыты) в предложении. Это полезно для понимания грамматических и семантических отношений между словами.\n",
    "Пример: unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "5. summarization (Суммаризация)\n",
    "Описание: Сокращает длинный текст, оставляя только ключевые идеи. Может быть экстрактивной (собирает важные предложения из исходного текста) или абстрактивной (генерирует новый текст, который резюмирует исходный).\n",
    "Пример: summarizer = pipeline(\"summarization\")\n",
    "\n",
    "6. translation (Перевод)\n",
    "Описание: Переводит текст с одного языка на другой. Для использования необходимо указать языковую пару, например, translation_en_to_ru (английский-русский).\n",
    "Пример: translator = pipeline(\"translation_en_to_ru\")\n",
    "\n",
    "7. text-generation (Генерация текста)\n",
    "Описание: Генерирует продолжение текста на основе предоставленной \"затравки\" (prompt).\n",
    "Пример: text_generator = pipeline(\"text-generation\")\n",
    "\n",
    "8. question-answering (Ответы на вопросы)\n",
    "Описание: Находит ответ на заданный вопрос в предоставленном контексте (тексте).\n",
    "Пример: qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "9. ner (Распознавание именованных сущностей)\n",
    "Описание: Находит и классифицирует именованные сущности в тексте, такие как имена людей, организации, даты и местоположения.\n",
    "Пример: ner_pipeline = pipeline(\"ner\")\n",
    "\n",
    "10. feature-extraction (Извлечение признаков)\n",
    "Описание: Получает векторное представление (эмбеддинг) для текста, что полезно для последующих задач, таких как кластеризация или поиск сходства.\n",
    "Пример: extractor = pipeline(\"feature-extraction\")\n",
    "\n",
    "11. Другие типы задач\n",
    "Существуют также менее распространённые, но очень полезные варианты, такие как:\n",
    "- audio-classification (Классификация аудио)\n",
    "- image-classification (Классификация изображений)\n",
    "- object-detection (Детекция объектов на изображении)\n",
    "- image-segmentation (Сегментация изображений)\n",
    "- token-classification (Классификация токенов)\n",
    "- automatic-speech-recognition (Автоматическое распознавание речи)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6942070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9946828484535217}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifer = pipeline(\"sentiment-analysis\")\n",
    "classifer(\"Human is angry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e96d880",
   "metadata": {},
   "source": [
    "Пример продолжениея текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e6525",
   "metadata": {},
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85667c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'My day start with the old joke.\\n\\n\\nWe will run out of time to read my next book on meditation meditation.\\n(All things'},\n",
       " {'generated_text': 'My day start with the usual routine. Then I will open my bag and start getting coffee. Then I will get my coffee on my mind. Then'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "generator(\n",
    "    \"My day start with\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707ad6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Мой день начинается с того, что я просыпаюсь в шесть утра. Я встаю с постели, делаю зарядку, умываюсь, чищу зубы'},\n",
       " {'generated_text': 'Мой день начинается с того, что я просыпаюсь в шесть утра. Я встаю с постели, делаю зарядку, завтракаю и отправляюсь на'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_token = \"hf_***\"\n",
    "#generator = pipeline(\"text-generation\", model=\"tinkoff-ai/ru-gpts-tiny\", token=my_token)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"Sberbank-AI/rugpt3large_based_on_gpt2\")\n",
    "generator(\n",
    "    \"Мой день начинается с\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077fb16",
   "metadata": {},
   "source": [
    "Распознование сущностей в тексте,типа имя, локация и тд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b9951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.8774506,\n",
       "  'word': 'Serg',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9996278,\n",
       "  'word': 'Moscow',\n",
       "  'start': 26,\n",
       "  'end': 32}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My mane is Serg, i'm from Moscow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d72b6",
   "metadata": {},
   "source": [
    "Перевод текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbfed3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\ds-vl2\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.conda\\envs\\ds-vl2\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Hey, how's it going?\"}]\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ru-en\")\n",
    "\n",
    "result = translator(\"Привет, как дела?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e78760b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\ds-vl2\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-ru. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Привет, как дела?'}]\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ru\")\n",
    "\n",
    "result = translator(\"Hello, how are you?\")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-vl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
